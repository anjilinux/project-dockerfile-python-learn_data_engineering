{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.kdnuggets.com/2016/06/regularization-logistic-regression.html\n",
    "\n",
    "https://github.com/dedupeio/dedupe/tree/master/docs\n",
    "\n",
    "https://github.com/dedupeio/dedupe-examples/tree/master/csv_example\n",
    "\n",
    "https://dedupeio.github.io/dedupe-examples/docs/csv_example.html\n",
    "\n",
    "https://dedupe.io/developers/library/en/latest/API-documentation.html#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preProcess(column):\n",
    "\n",
    "#\n",
    "\n",
    "    try : # python 2/3 string differences\n",
    "        column = column.decode('utf8')\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "\n",
    "#If data is missing, indicate that by setting the value to None\n",
    "\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "#Read in our data from a CSV file and create a dictionary of records, where the key is a unique record ID and each value is dict\n",
    "\n",
    "def readData(filename):\n",
    "\n",
    "#\n",
    "\n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['Id'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "print('importing data ...')\n",
    "data_dd = readData(path+'\\\\raw.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dd[0]['Address']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(path+'\\\\raw.txt', sep=',', index_col=None, encoding='utf-8', \n",
    "                 dtype={'Site name':'object', 'Address':'object',\n",
    "                        'Zip':'float', 'Phone':'float'})\n",
    "print(len(raw))\n",
    "match_cols =  ['Site name', 'Address', 'Zip', 'Phone']\n",
    "print(raw[match_cols].dtypes)\n",
    "print(raw[match_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_raw(df, cols):\n",
    "    for col in cols:\n",
    "        if df[col].dtype ==  np.object_:\n",
    "            df[col+'_'] = df[col].copy().str.lower().str.replace('[^a-zA-Z0-9]', ' ').copy()\n",
    "        else:\n",
    "            df[col+'_'] = df[col].copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleansed = cleanse_raw(raw, match_cols)\n",
    "clean_cols = [x for x in cleansed.columns if '_' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = cleansed[clean_cols].to_dict(orient='index')\n",
    "data_d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d[k].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data_d.keys():\n",
    "    for k1 in ['Address_', 'Site name_']:\n",
    "        data_d[k][k1] = str(data_d[k][k1])\n",
    "    for k2 in ['Zip_', 'Phone_']:\n",
    "        if np.isnan(data_d[k][k2]):\n",
    "            data_d[k][k2] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fields to use for de-dedup\n",
    "fields = [\n",
    "    {'field' : 'Site name', 'type': 'String'},\n",
    "    {'field' : 'Address', 'type': 'String'},\n",
    "    {'field' : 'Zip', 'type': 'Exact', 'has missing' : True},\n",
    "    {'field' : 'Phone', 'type': 'String', 'has missing' : True},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new deduper object and pass our data model to it.\n",
    "deduper = dedupe.Dedupe(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.sample(data_dd, sample_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe.consoleLabel(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = deduper.threshold(data_dd, recall_weight=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = '\\\\csv_example_output.csv'\n",
    "settings_file = '\\\\csv_example_learned_settings'\n",
    "training_file = '\\\\csv_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(path+training_file, 'w') as tf:\n",
    "    deduper.writeTraining(tf)\n",
    "\n",
    "#Save our weights and predicates to disk. If the settings file exists, we will skip all the training and learning next time we run this file.\n",
    "\n",
    "with open(path+settings_file, 'wb') as sf:\n",
    "    deduper.writeSettings(sf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('clustering...')\n",
    "clustered_dupes = deduper.match(data_dd, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dupes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dd[215]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "3430592c81e614f3eade9b786217ce459fc9e80474b1d5165e0a306b9cf201f8"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}